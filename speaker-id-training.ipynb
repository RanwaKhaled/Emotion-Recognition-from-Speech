{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13901773,"sourceType":"datasetVersion","datasetId":8856692}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install speechbrain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T23:43:10.083208Z","iopub.execute_input":"2025-11-27T23:43:10.083520Z","iopub.status.idle":"2025-11-27T23:43:10.087158Z","shell.execute_reply.started":"2025-11-27T23:43:10.083499Z","shell.execute_reply":"2025-11-27T23:43:10.086403Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:17:19.417000Z","iopub.execute_input":"2025-11-28T00:17:19.417690Z","iopub.status.idle":"2025-11-28T00:17:19.423465Z","shell.execute_reply.started":"2025-11-28T00:17:19.417655Z","shell.execute_reply":"2025-11-28T00:17:19.422659Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"# load the dataset\ndata_path = \"/kaggle/input/speakerid-dataset\"\naudios = os.listdir(data_path)\naudios.remove(\"test_audio_ranwa.wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:27.885200Z","iopub.execute_input":"2025-11-28T00:21:27.885974Z","iopub.status.idle":"2025-11-28T00:21:27.891850Z","shell.execute_reply.started":"2025-11-28T00:21:27.885947Z","shell.execute_reply":"2025-11-28T00:21:27.891271Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"import torchaudio\nimport torch\nfrom speechbrain.inference.speaker import EncoderClassifier\n\n# Load embeddings model\nclassifier = EncoderClassifier.from_hparams(\n    source=\"speechbrain/spkrec-ecapa-voxceleb\",\n    run_opts={\"device\": \"cuda\"}   # <--- THIS PUTS THE MODEL ON GPU\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:45.109883Z","iopub.execute_input":"2025-11-28T00:21:45.110611Z","iopub.status.idle":"2025-11-28T00:21:45.991977Z","shell.execute_reply.started":"2025-11-28T00:21:45.110585Z","shell.execute_reply":"2025-11-28T00:21:45.991191Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"def get_audio_embeddings(data_path, audios):\n    data = []\n    for audio in tqdm(audios, total=len(audios)):\n        # Load audio\n        signal, fs = torchaudio.load(os.path.join(data_path,audio))\n        signal = torch.mean(signal, dim=0, keepdim=True)  # now shape [1, num_samples] - convert stereo to mono\n        \n        # Move signal to GPU\n        signal = signal.to(\"cuda\")\n        \n        # Compute embeddings\n        embeddings = classifier.encode_batch(signal)\n\n        # convert to list\n        embeddings = embeddings.squeeze()                         # -> (192,)\n        embeddings_list = embeddings.cpu().tolist()\n        data.append((\"_\".join(audio.split(\"_\")[:2]), embeddings_list)) \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:45.993044Z","iopub.execute_input":"2025-11-28T00:21:45.993654Z","iopub.status.idle":"2025-11-28T00:21:45.998428Z","shell.execute_reply.started":"2025-11-28T00:21:45.993633Z","shell.execute_reply":"2025-11-28T00:21:45.997703Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"train_set = get_audio_embeddings(data_path, audios)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:45.999176Z","iopub.execute_input":"2025-11-28T00:21:45.999356Z","iopub.status.idle":"2025-11-28T00:21:51.248621Z","shell.execute_reply.started":"2025-11-28T00:21:45.999341Z","shell.execute_reply":"2025-11-28T00:21:51.247760Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 180/180 [00:05<00:00, 34.44it/s]\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"# shuffle our data\nrandom.shuffle(train_set)\n\n# separate target and features\nx = [sample[1] for sample in train_set]\ny = [sample[0] for sample in train_set]\n\n# reduce size of embedding array\npca = PCA(n_components=20)\nx_pca = pca.fit_transform(list(x))  # from 192 to 20 parameters\n\n# encode the labels \nname2idx = {\"ranwa_khaled\": 0, \"nour_adel\": 1, \"nour_nader\": 2}\nidx2name = {0:\"ranwa_khaled\", 1:\"nour_adel\", 2:\"nour_nader\"}\n\ny = [name2idx[name] for name in y]\n\n# 90% train, 10% test\nx_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.1, random_state=42)\n\n# train logistic regression\nLRmodel = LogisticRegression()\nLRmodel.fit(x_train, y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:51.250060Z","iopub.execute_input":"2025-11-28T00:21:51.250277Z","iopub.status.idle":"2025-11-28T00:21:51.298181Z","shell.execute_reply.started":"2025-11-28T00:21:51.250261Z","shell.execute_reply":"2025-11-28T00:21:51.297605Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ny_pred = LRmodel.predict(x_test)\naccuracy_score(y_pred, y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:51.298633Z","iopub.execute_input":"2025-11-28T00:21:51.298829Z","iopub.status.idle":"2025-11-28T00:21:51.310613Z","shell.execute_reply.started":"2025-11-28T00:21:51.298812Z","shell.execute_reply":"2025-11-28T00:21:51.310049Z"}},"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"1.0"},"metadata":{}}],"execution_count":88},{"cell_type":"code","source":"# Save the lr model to a file\nfilename = 'speakerID_model.pkl'\npickle.dump(LRmodel, open(filename, 'wb'))\n\n# To load the model later\nloaded_model = pickle.load(open(filename, 'rb'))\n# Now you can use loaded_model for predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:51.311093Z","iopub.execute_input":"2025-11-28T00:21:51.311295Z","iopub.status.idle":"2025-11-28T00:21:51.323630Z","shell.execute_reply.started":"2025-11-28T00:21:51.311278Z","shell.execute_reply":"2025-11-28T00:21:51.322896Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# 2. Save the trained PCA model\nfilename = 'pca_model.pkl'\nwith open(filename, 'wb') as file:\n    pickle.dump(pca, file)\n\nprint(f\"PCA model saved to {filename}\")\n\n# 3. Load the saved PCA model (in a new session or script)\nwith open(filename, 'rb') as file:\n    loaded_pca = pickle.load(file)\n\nprint(\"PCA model loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:51.503400Z","iopub.execute_input":"2025-11-28T00:21:51.503631Z","iopub.status.idle":"2025-11-28T00:21:51.509224Z","shell.execute_reply.started":"2025-11-28T00:21:51.503615Z","shell.execute_reply":"2025-11-28T00:21:51.508558Z"}},"outputs":[{"name":"stdout","text":"PCA model saved to pca_model.pkl\nPCA model loaded successfully.\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"LRmodel.predict([x_test[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:21:52.460572Z","iopub.execute_input":"2025-11-28T00:21:52.461161Z","iopub.status.idle":"2025-11-28T00:21:52.466290Z","shell.execute_reply.started":"2025-11-28T00:21:52.461134Z","shell.execute_reply":"2025-11-28T00:21:52.465663Z"}},"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"array([2])"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"sample_path = \"/kaggle/input/speakerid-dataset/test_audio_ranwa.wav\"\n\ndef prep_audio_sample(audio):\n    # Load audio\n    signal, fs = torchaudio.load(audio)\n    signal = torch.mean(signal, dim=0, keepdim=True)  # now shape [1, num_samples] - convert stereo to mono\n    \n    # Move signal to GPU\n    signal = signal.to(\"cuda\")\n    \n    # Compute embeddings\n    embeddings = classifier.encode_batch(signal)\n\n    # convert to list\n    embeddings = embeddings.squeeze()                         # -> (192,)\n    embeddings_list = embeddings.cpu().tolist()\n    \n    return pca.transform([embeddings_list])\n    \nprediction = LRmodel.predict(prep_audio_sample(sample_path))\n\nprint(idx2name[prediction[0]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T00:23:34.871265Z","iopub.execute_input":"2025-11-28T00:23:34.871966Z","iopub.status.idle":"2025-11-28T00:23:34.926988Z","shell.execute_reply.started":"2025-11-28T00:23:34.871941Z","shell.execute_reply":"2025-11-28T00:23:34.926094Z"}},"outputs":[{"name":"stdout","text":"ranwa_khaled\n","output_type":"stream"}],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}